{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Information Retrieval assignment 1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMPxO3xV1Xg5NwlDR4NOpkP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GurkiratSarna/CE888-Decision-Making-UoE/blob/master/Information_Retrieval_assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn3kCZ8YQWew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from requests import get"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_XPlDiqkx5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qycet6YQUhD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_url(input_url):\n",
        "  response=get(input_url)\n",
        "  return response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94zsttrlVBGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response=read_url(\"http://www.multimediaeval.org/mediaeval2019/memorability/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYCFcuT5VJ2p",
        "colab_type": "code",
        "outputId": "6d566c69-56f4-413c-c1c1-5e741bbbdcd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(response.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n",
            "<html xmlns=\"http://www.w3.org/1999/xhtml\">\n",
            "\t<head>\n",
            "\t\t<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
            "\t\t<meta name=\"generator\" content=\"RapidWeaver\" />\n",
            "\t\t\n",
            "\t\t<title>Media Memorability</title>\n",
            "\t\t<link rel=\"stylesheet\" type=\"text/css\" media=\"print\" href=\"../../rw_common/themes/simple/consolidated-print-114.css\" />\n",
            "\t\t<link rel=\"stylesheet\" type=\"text/css\" media=\"screen\" href=\"../../rw_common/themes/simple/consolidated-screen-114.css\" />\n",
            "\t\t<link rel=\"stylesheet\" type=\"text/css\" media=\"handheld\" href=\"../../rw_common/themes/simple/consolidated-handheld-114.css\" />\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t<script type=\"text/javascript\" src=\"../../rw_common/themes/simple/javascript.js\"></script>\n",
            "\t\t\n",
            "\t\t\n",
            "\t\t\n",
            "\t</head>\n",
            "<body>\n",
            "<div id=\"container\"><!-- Start container -->\n",
            "\t\n",
            "\t<div id=\"pageHeader\"><!-- Start page header -->\n",
            "\t\t\n",
            "\t\t<h1>MediaEval Benchmarking Initiative for Multimedia Evaluation</h1>\n",
            "\t\t<h2>The \"multi\" in multimedia: speech, audio, visual content, tags, users, context</h2>\n",
            "\t</div><!-- End page header -->\n",
            "\t\n",
            "\t<div id=\"sidebarContainer\"><!-- Start Sidebar wrapper -->\n",
            "\t\t<div id=\"navcontainer\"><!-- Start Navigation -->\n",
            "\t\t\t<ul><li><a href=\"../../\" rel=\"self\">Home</a></li><li><a href=\"../../about/\" rel=\"self\">About MediaEval</a></li><li><a href=\"../../datasets/\" rel=\"self\">Datasets</a></li><li><a href=\"../../mediaeval2019/\" rel=\"self\" class=\"currentAncestor\">MediaEval 2019</a><ul><li><a href=\"../../mediaeval2019/music/\" rel=\"self\">Emotion & Themes in Music</a></li><li><a href=\"../../mediaeval2019/eyesears/\" rel=\"self\">Eyes & Ears Together</a></li><li><a href=\"../../mediaeval2019/gamestory/\" rel=\"self\">GameStory</a></li><li><a href=\"../../mediaeval2019/wellbeing/\" rel=\"self\">Lifelogging for wellbeing</a></li><li><a href=\"../../mediaeval2019/medico/\" rel=\"self\">Medico Multimedia</a></li><li><a href=\"../../mediaeval2019/mmrecsys/\" rel=\"self\">Multimedia RecSys</a></li><li><a href=\"../../mediaeval2019/multimediasatellite/\" rel=\"self\">Multimedia Satellite</a></li><li><a href=\"../../mediaeval2019/speakerturns/\" rel=\"self\">No-audio speech turns</a></li><li><a href=\"../../mediaeval2019/pixelprivacy/\" rel=\"self\">Pixel Privacy</a></li><li><a href=\"./\" rel=\"self\" id=\"current\">Media Memorability</a></li><li><a href=\"../../mediaeval2019/scenechange/\" rel=\"self\">Scene Change</a></li><li><a href=\"../../mediaeval2019/sports/\" rel=\"self\">Sports video</a></li></ul></li><li><a href=\"../../mediaeval2018/\" rel=\"self\">MediaEval 2018</a></li><li><a href=\"../../mediaeval2017/\" rel=\"self\">MediaEval 2017</a></li><li><a href=\"../../mediaeval2016/\" rel=\"self\">MediaEval 2016</a></li><li><a href=\"../../mediaeval2015/\" rel=\"self\">MediaEval 2015</a></li><li><a href=\"../../mediaeval2014/\" rel=\"self\">MediaEval 2014</a></li><li><a href=\"../../mediaeval2013/\" rel=\"self\">MediaEval 2013</a></li><li><a href=\"../../mediaeval2012/\" rel=\"self\">MediaEval 2012</a></li><li><a href=\"../../mediaeval2011/\" rel=\"self\">MediaEval 2011</a></li><li><a href=\"../../mediaeval2010/\" rel=\"self\">MediaEval 2010</a></li><li><a href=\"../../videoclef09/\" rel=\"self\">VideoCLEF 2009</a></li><li><a href=\"../../videoclef08/\" rel=\"self\">VideoCLEF 2008</a></li><li><a href=\"../../video/\" rel=\"self\">Videos about MediaEval</a></li><li><a href=\"../../why/\" rel=\"self\">Why Participate?</a></li><li><a href=\"../../who/\" rel=\"self\">Who are we?</a></li><li><a href=\"../../openscience/\" rel=\"self\">Open Science</a></li><li><a href=\"../../thanks/\" rel=\"self\">Acknowledgments</a></li></ul>\n",
            "\t\t</div><!-- End navigation --> \n",
            "\t\t<div id=\"sidebar\"><!-- Start sidebar content -->\n",
            "\t\t\t<h1 class=\"sideHeader\"></h1><!-- Sidebar header -->\n",
            "\t\t\t<!-- sidebar content you enter in the page inspector -->\n",
            "\t\t\t <!-- sidebar content such as the blog archive links -->\n",
            "\t\t</div><!-- End sidebar content -->\n",
            "\t</div><!-- End sidebar wrapper -->\n",
            "\t\n",
            "\t<div id=\"contentContainer\"><!-- Start main content wrapper -->\n",
            "\t\t<div id=\"content\"><!-- Start content -->\n",
            "\t\t\t<span style=\"font-size:14px; font-weight:bold; \">The 2019 Predicting Media Memorability Task<br /></span><span style=\"font-size:14px; \"><br /></span><strong>Task description</strong><br />This task focuses on the problem of predicting how memorable a video is to viewers. It requires participants to automatically predict memorability scores for videos that reflect the probability a video will be remembered. Task participants are provided with an extensive dataset of videos that are accompanied by memorability annotations, as well as pre-extracted state-of-the-art visual features. The ground truth has been collected through recognition tests, and thus results from objective measurement of memory performance. Participants will be required to train computational models capable of inferring video memorability from visual content. Optionally, descriptive titles attached to the videos may be used. Models will be evaluated through standard evaluation metrics used in ranking tasks (Spearman&rsquo;s rank correlation). The data set used in 2019, is the same as in 2018 (2018&rsquo;s testset ground truth data has not been released). This year the task focuses on understanding the patterns in the data and improving the ability of algorithms to capture those patterns.<br /><br /><strong>Task motivation and background</strong><br />The motivation for the task is the growth over recent years in the amount of multimedia content shared and consumed via online platforms. Enhancing the usefulness of multimedia requires new ways to organize&ndash;in particular, to recommend and retrieve&ndash;digital content. Like other aspects related to video relevance, such as aesthetics or interestingness, memorability can be regarded as useful to help make a choice between competing videos. Effective memorability prediction models will also push forward the semantic understanding of multimedia content by putting human cognition and perception at the center of the multimedia analysis.<br /><br />A number of different application areas benefit from deeper understanding of what makes some videos memorable, and others less so. These areas include recommendation and retrieval in online platforms, already mentioned, but also advertising, filmmaking, and education. It is increasingly important to understand what makes videos memorable in order to keep the use of automatic processing techniques evenhanded. Applications can make use of information about predicted memorability, but it is important to understand memorability well enough to be able to avoid systems that are hyper-optimized to viewer responses. <br /><br /><strong>Target group</strong><br />Researchers will find this task interesting if they work in the areas of human perception and the impact of multimedia on perception such as image and video interestingness, memorability, attractiveness, aesthetics prediction, event detection, multimedia affect and perceptual analysis, multimedia content analysis, machine learning (though not limited to).<br /><br /><strong>Data</strong><br />The dataset is composed of 10,000 (soundless) short videos extracted from raw footage used by professionals when creating content, and in particular, commercials. Each video consists of a coherent unit in terms of meaning and is associated with two scores of memorability that refer to its probability to be remembered after two different durations of memory retention. <br /><br />The videos are shared under Creative Commons licenses that allow their redistribution. They come with a set of pre-extracted features, such as: Dense SIFT, HoG descriptors, LBP, GIST, Color Histogram, MFCC, Fc7 layer from AlexNet, C3D features, etc.<br /><br /><strong>Ground truth and evaluation</strong><br />Each video consists of a coherent unit in terms of meaning and is associated with two scores of memorability that refer to its probability to be remembered after two different durations of memory retention. Memorability has been measured using recognition tests, i.e., through an objective measure, a few minutes after the memorization of the videos, and then 24 to 72 hours later.<br /><br />The outputs of the prediction models &ndash; i.e., the predicted memorability scores for the videos &ndash; will be compared with ground truth memorability scores using classic evaluation metrics (e.g., Spearman&rsquo;s rank correlation).<br /><br /><strong>Recommended reading</strong><br />[1] Romain Cohendet, Claire-H&eacute;l&egrave;ne Demarty, Ngoc Q. K. Duong, Mats Sj&ouml;berg, Bogdan Ionescu, Thanh-Toan Do. 2018. MediaEval 2018: Predicting Media Memorability. In Working Notes Proceedings of the MediaEval 2018 Workshop. Sophia Antipolis, France, 29-31 October 2018.<br />[2] Aditya Khosla, Akhil S Raju, Antonio Torralba, and Aude Oliva. 2015. Understanding and predicting image memorability at a large scale. In Proc. IEEE Int. Conf. on Computer Vision (ICCV). 2390&ndash;2398.<br />[3] Phillip Isola, Jianxiong Xiao, Devi Parikh, Antonio Torralba, and Aude Oliva. 2014. What makes a photograph memorable? IEEE Transactions on Pattern Analysis and Machine Intelligence 36, 7 (2014), 1469&ndash;1482.<br />[4] Hammad Squalli-Houssaini, Ngoc Duong, Marquant Gwena&euml;lle, and Claire-H&eacute;l&egrave;ne Demarty. 2018. Deep learning for predicting image memorability. In Proc. IEEE Int. Conf. on Audio, Speech and Language Processing (ICASSP).<br />[5] Junwei Han, Changyuan Chen, Ling Shao, Xintao Hu, Jungong Han, and Tianming Liu. 2015. Learning computational models of video memorability from fMRI brain imaging. IEEE transactions on cybernetics 45, 8 (2015), 1692&ndash;1703.<br />[6] Sumit Shekhar, Dhruv Singal, Harvineet Singh, Manav Kedia, and Akhil Shetty. 2017. Show and Recall: Learning What Makes Videos Memorable. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2730&ndash;2739.<br />[7] R. Cohendet, K. Yadati, N. Duong, C-H. Demarty. 2018. Annotating, understanding, and predicting long-term video memorability. In proc. ACM Int. Conf. on Multimedia Retrieval (ICMR).<br />[8] Romain Cohendet, Claire-H&eacute;l&egrave;ne Demarty, Ngoc Q. K. Duong, M. Engilberge. VideoMem: Constructing, Analyzing, Predicting Short-term and Long-term Video Memorability. 2018. Arxiv:1812.01973<br /><br />Please note that a dataset for predicting long-term video memorability is publicly available with [7] at the following address: <br /><a href=\"https://www.technicolor.com/dream/research-innovation/movie-memorability-dataset\" rel=\"self\">https://www.technicolor.com/dream/research-innovation/movie-memorability-dataset</a><br /><br /><strong>Task organizers</strong><br />Mihai Gabriel Constantin, University Politehnica of Bucharest, Romania<br />Bogdan Ionescu, University Politehnica of Bucharest, Romania<br />Claire-H&eacute;l&egrave;ne Demarty, Technicolor, France <br />Quang-Khanh-Ngoc Duong, Technicolor, France <br />Xavier Alameda-Pineda, INRIA, France<br />Mats Sj&ouml;berg, CSC, Finland<br /><br /><strong>Task auxiliaries</strong><br />Liviu-Daniel Åtefan, University Politehnica of Bucharest, Romania<br />Ricardo Savii, Federal University of S&atilde;o Paulo, Brazil<br /><br /><strong>Task schedule</strong><br />Development data release: 1 May 2019<br />Test data release: 03 June 2019<br />Runs due: 20 September 2019<br />Working Notes paper due: 30 September 2019 <br />MediaEval 2019 Workshop (in France, near Nice): 27-29 October 2019<br />\n",
            "\t\t</div><!-- End content -->\n",
            "\t</div><!-- End main content wrapper -->\n",
            "\t\n",
            "\t<div class=\"clearer\"></div>\n",
            "\t\n",
            "\t<div id=\"footer\"><!-- Start Footer -->\n",
            "\t\t<div id=\"breadcrumbcontainer\"><!-- Start the breadcrumb wrapper -->\n",
            "\t\t\t\n",
            "\t\t</div><!-- End breadcrumb -->\n",
            "\t\t<p>&copy; 2020 MediaEval Multimedia Benchmark <a href=\"#\" id=\"rw_email_contact\">Contact</a><script type=\"text/javascript\">var _rwObsfuscatedHref0 = \"mai\";var _rwObsfuscatedHref1 = \"lto\";var _rwObsfuscatedHref2 = \":m.\";var _rwObsfuscatedHref3 = \"a.l\";var _rwObsfuscatedHref4 = \"ars\";var _rwObsfuscatedHref5 = \"on@\";var _rwObsfuscatedHref6 = \"tud\";var _rwObsfuscatedHref7 = \"elf\";var _rwObsfuscatedHref8 = \"t.n\";var _rwObsfuscatedHref9 = \"l\";var _rwObsfuscatedHref = _rwObsfuscatedHref0+_rwObsfuscatedHref1+_rwObsfuscatedHref2+_rwObsfuscatedHref3+_rwObsfuscatedHref4+_rwObsfuscatedHref5+_rwObsfuscatedHref6+_rwObsfuscatedHref7+_rwObsfuscatedHref8+_rwObsfuscatedHref9; document.getElementById('rw_email_contact').href = _rwObsfuscatedHref;</script></p>\n",
            "\t</div><!-- End Footer -->\n",
            "\n",
            "</div><!-- End container -->\n",
            "</body>\n",
            "</html>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqt_zjBxVLFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "html_soup= BeautifulSoup(response.text, 'html.parser')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcOc-urZV3oi",
        "colab_type": "code",
        "outputId": "e5b16a41-756e-467a-e4ae-8a09dc08e483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "html_soup"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n",
              "\n",
              "<html xmlns=\"http://www.w3.org/1999/xhtml\">\n",
              "<head>\n",
              "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
              "<meta content=\"RapidWeaver\" name=\"generator\"/>\n",
              "<title>Media Memorability</title>\n",
              "<link href=\"../../rw_common/themes/simple/consolidated-print-114.css\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n",
              "<link href=\"../../rw_common/themes/simple/consolidated-screen-114.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n",
              "<link href=\"../../rw_common/themes/simple/consolidated-handheld-114.css\" media=\"handheld\" rel=\"stylesheet\" type=\"text/css\"/>\n",
              "<script src=\"../../rw_common/themes/simple/javascript.js\" type=\"text/javascript\"></script>\n",
              "</head>\n",
              "<body>\n",
              "<div id=\"container\"><!-- Start container -->\n",
              "<div id=\"pageHeader\"><!-- Start page header -->\n",
              "<h1>MediaEval Benchmarking Initiative for Multimedia Evaluation</h1>\n",
              "<h2>The \"multi\" in multimedia: speech, audio, visual content, tags, users, context</h2>\n",
              "</div><!-- End page header -->\n",
              "<div id=\"sidebarContainer\"><!-- Start Sidebar wrapper -->\n",
              "<div id=\"navcontainer\"><!-- Start Navigation -->\n",
              "<ul><li><a href=\"../../\" rel=\"self\">Home</a></li><li><a href=\"../../about/\" rel=\"self\">About MediaEval</a></li><li><a href=\"../../datasets/\" rel=\"self\">Datasets</a></li><li><a class=\"currentAncestor\" href=\"../../mediaeval2019/\" rel=\"self\">MediaEval 2019</a><ul><li><a href=\"../../mediaeval2019/music/\" rel=\"self\">Emotion &amp; Themes in Music</a></li><li><a href=\"../../mediaeval2019/eyesears/\" rel=\"self\">Eyes &amp; Ears Together</a></li><li><a href=\"../../mediaeval2019/gamestory/\" rel=\"self\">GameStory</a></li><li><a href=\"../../mediaeval2019/wellbeing/\" rel=\"self\">Lifelogging for wellbeing</a></li><li><a href=\"../../mediaeval2019/medico/\" rel=\"self\">Medico Multimedia</a></li><li><a href=\"../../mediaeval2019/mmrecsys/\" rel=\"self\">Multimedia RecSys</a></li><li><a href=\"../../mediaeval2019/multimediasatellite/\" rel=\"self\">Multimedia Satellite</a></li><li><a href=\"../../mediaeval2019/speakerturns/\" rel=\"self\">No-audio speech turns</a></li><li><a href=\"../../mediaeval2019/pixelprivacy/\" rel=\"self\">Pixel Privacy</a></li><li><a href=\"./\" id=\"current\" rel=\"self\">Media Memorability</a></li><li><a href=\"../../mediaeval2019/scenechange/\" rel=\"self\">Scene Change</a></li><li><a href=\"../../mediaeval2019/sports/\" rel=\"self\">Sports video</a></li></ul></li><li><a href=\"../../mediaeval2018/\" rel=\"self\">MediaEval 2018</a></li><li><a href=\"../../mediaeval2017/\" rel=\"self\">MediaEval 2017</a></li><li><a href=\"../../mediaeval2016/\" rel=\"self\">MediaEval 2016</a></li><li><a href=\"../../mediaeval2015/\" rel=\"self\">MediaEval 2015</a></li><li><a href=\"../../mediaeval2014/\" rel=\"self\">MediaEval 2014</a></li><li><a href=\"../../mediaeval2013/\" rel=\"self\">MediaEval 2013</a></li><li><a href=\"../../mediaeval2012/\" rel=\"self\">MediaEval 2012</a></li><li><a href=\"../../mediaeval2011/\" rel=\"self\">MediaEval 2011</a></li><li><a href=\"../../mediaeval2010/\" rel=\"self\">MediaEval 2010</a></li><li><a href=\"../../videoclef09/\" rel=\"self\">VideoCLEF 2009</a></li><li><a href=\"../../videoclef08/\" rel=\"self\">VideoCLEF 2008</a></li><li><a href=\"../../video/\" rel=\"self\">Videos about MediaEval</a></li><li><a href=\"../../why/\" rel=\"self\">Why Participate?</a></li><li><a href=\"../../who/\" rel=\"self\">Who are we?</a></li><li><a href=\"../../openscience/\" rel=\"self\">Open Science</a></li><li><a href=\"../../thanks/\" rel=\"self\">Acknowledgments</a></li></ul>\n",
              "</div><!-- End navigation -->\n",
              "<div id=\"sidebar\"><!-- Start sidebar content -->\n",
              "<h1 class=\"sideHeader\"></h1><!-- Sidebar header -->\n",
              "<!-- sidebar content you enter in the page inspector -->\n",
              "<!-- sidebar content such as the blog archive links -->\n",
              "</div><!-- End sidebar content -->\n",
              "</div><!-- End sidebar wrapper -->\n",
              "<div id=\"contentContainer\"><!-- Start main content wrapper -->\n",
              "<div id=\"content\"><!-- Start content -->\n",
              "<span style=\"font-size:14px; font-weight:bold; \">The 2019 Predicting Media Memorability Task<br/></span><span style=\"font-size:14px; \"><br/></span><strong>Task description</strong><br/>This task focuses on the problem of predicting how memorable a video is to viewers. It requires participants to automatically predict memorability scores for videos that reflect the probability a video will be remembered. Task participants are provided with an extensive dataset of videos that are accompanied by memorability annotations, as well as pre-extracted state-of-the-art visual features. The ground truth has been collected through recognition tests, and thus results from objective measurement of memory performance. Participants will be required to train computational models capable of inferring video memorability from visual content. Optionally, descriptive titles attached to the videos may be used. Models will be evaluated through standard evaluation metrics used in ranking tasks (Spearman’s rank correlation). The data set used in 2019, is the same as in 2018 (2018’s testset ground truth data has not been released). This year the task focuses on understanding the patterns in the data and improving the ability of algorithms to capture those patterns.<br/><br/><strong>Task motivation and background</strong><br/>The motivation for the task is the growth over recent years in the amount of multimedia content shared and consumed via online platforms. Enhancing the usefulness of multimedia requires new ways to organize–in particular, to recommend and retrieve–digital content. Like other aspects related to video relevance, such as aesthetics or interestingness, memorability can be regarded as useful to help make a choice between competing videos. Effective memorability prediction models will also push forward the semantic understanding of multimedia content by putting human cognition and perception at the center of the multimedia analysis.<br/><br/>A number of different application areas benefit from deeper understanding of what makes some videos memorable, and others less so. These areas include recommendation and retrieval in online platforms, already mentioned, but also advertising, filmmaking, and education. It is increasingly important to understand what makes videos memorable in order to keep the use of automatic processing techniques evenhanded. Applications can make use of information about predicted memorability, but it is important to understand memorability well enough to be able to avoid systems that are hyper-optimized to viewer responses. <br/><br/><strong>Target group</strong><br/>Researchers will find this task interesting if they work in the areas of human perception and the impact of multimedia on perception such as image and video interestingness, memorability, attractiveness, aesthetics prediction, event detection, multimedia affect and perceptual analysis, multimedia content analysis, machine learning (though not limited to).<br/><br/><strong>Data</strong><br/>The dataset is composed of 10,000 (soundless) short videos extracted from raw footage used by professionals when creating content, and in particular, commercials. Each video consists of a coherent unit in terms of meaning and is associated with two scores of memorability that refer to its probability to be remembered after two different durations of memory retention. <br/><br/>The videos are shared under Creative Commons licenses that allow their redistribution. They come with a set of pre-extracted features, such as: Dense SIFT, HoG descriptors, LBP, GIST, Color Histogram, MFCC, Fc7 layer from AlexNet, C3D features, etc.<br/><br/><strong>Ground truth and evaluation</strong><br/>Each video consists of a coherent unit in terms of meaning and is associated with two scores of memorability that refer to its probability to be remembered after two different durations of memory retention. Memorability has been measured using recognition tests, i.e., through an objective measure, a few minutes after the memorization of the videos, and then 24 to 72 hours later.<br/><br/>The outputs of the prediction models – i.e., the predicted memorability scores for the videos – will be compared with ground truth memorability scores using classic evaluation metrics (e.g., Spearman’s rank correlation).<br/><br/><strong>Recommended reading</strong><br/>[1] Romain Cohendet, Claire-Hélène Demarty, Ngoc Q. K. Duong, Mats Sjöberg, Bogdan Ionescu, Thanh-Toan Do. 2018. MediaEval 2018: Predicting Media Memorability. In Working Notes Proceedings of the MediaEval 2018 Workshop. Sophia Antipolis, France, 29-31 October 2018.<br/>[2] Aditya Khosla, Akhil S Raju, Antonio Torralba, and Aude Oliva. 2015. Understanding and predicting image memorability at a large scale. In Proc. IEEE Int. Conf. on Computer Vision (ICCV). 2390–2398.<br/>[3] Phillip Isola, Jianxiong Xiao, Devi Parikh, Antonio Torralba, and Aude Oliva. 2014. What makes a photograph memorable? IEEE Transactions on Pattern Analysis and Machine Intelligence 36, 7 (2014), 1469–1482.<br/>[4] Hammad Squalli-Houssaini, Ngoc Duong, Marquant Gwenaëlle, and Claire-Hélène Demarty. 2018. Deep learning for predicting image memorability. In Proc. IEEE Int. Conf. on Audio, Speech and Language Processing (ICASSP).<br/>[5] Junwei Han, Changyuan Chen, Ling Shao, Xintao Hu, Jungong Han, and Tianming Liu. 2015. Learning computational models of video memorability from fMRI brain imaging. IEEE transactions on cybernetics 45, 8 (2015), 1692–1703.<br/>[6] Sumit Shekhar, Dhruv Singal, Harvineet Singh, Manav Kedia, and Akhil Shetty. 2017. Show and Recall: Learning What Makes Videos Memorable. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2730–2739.<br/>[7] R. Cohendet, K. Yadati, N. Duong, C-H. Demarty. 2018. Annotating, understanding, and predicting long-term video memorability. In proc. ACM Int. Conf. on Multimedia Retrieval (ICMR).<br/>[8] Romain Cohendet, Claire-Hélène Demarty, Ngoc Q. K. Duong, M. Engilberge. VideoMem: Constructing, Analyzing, Predicting Short-term and Long-term Video Memorability. 2018. Arxiv:1812.01973<br/><br/>Please note that a dataset for predicting long-term video memorability is publicly available with [7] at the following address: <br/><a href=\"https://www.technicolor.com/dream/research-innovation/movie-memorability-dataset\" rel=\"self\">https://www.technicolor.com/dream/research-innovation/movie-memorability-dataset</a><br/><br/><strong>Task organizers</strong><br/>Mihai Gabriel Constantin, University Politehnica of Bucharest, Romania<br/>Bogdan Ionescu, University Politehnica of Bucharest, Romania<br/>Claire-Hélène Demarty, Technicolor, France <br/>Quang-Khanh-Ngoc Duong, Technicolor, France <br/>Xavier Alameda-Pineda, INRIA, France<br/>Mats Sjöberg, CSC, Finland<br/><br/><strong>Task auxiliaries</strong><br/>Liviu-Daniel Åtefan, University Politehnica of Bucharest, Romania<br/>Ricardo Savii, Federal University of São Paulo, Brazil<br/><br/><strong>Task schedule</strong><br/>Development data release: 1 May 2019<br/>Test data release: 03 June 2019<br/>Runs due: 20 September 2019<br/>Working Notes paper due: 30 September 2019 <br/>MediaEval 2019 Workshop (in France, near Nice): 27-29 October 2019<br/>\n",
              "</div><!-- End content -->\n",
              "</div><!-- End main content wrapper -->\n",
              "<div class=\"clearer\"></div>\n",
              "<div id=\"footer\"><!-- Start Footer -->\n",
              "<div id=\"breadcrumbcontainer\"><!-- Start the breadcrumb wrapper -->\n",
              "</div><!-- End breadcrumb -->\n",
              "<p>© 2020 MediaEval Multimedia Benchmark <a href=\"#\" id=\"rw_email_contact\">Contact</a><script type=\"text/javascript\">var _rwObsfuscatedHref0 = \"mai\";var _rwObsfuscatedHref1 = \"lto\";var _rwObsfuscatedHref2 = \":m.\";var _rwObsfuscatedHref3 = \"a.l\";var _rwObsfuscatedHref4 = \"ars\";var _rwObsfuscatedHref5 = \"on@\";var _rwObsfuscatedHref6 = \"tud\";var _rwObsfuscatedHref7 = \"elf\";var _rwObsfuscatedHref8 = \"t.n\";var _rwObsfuscatedHref9 = \"l\";var _rwObsfuscatedHref = _rwObsfuscatedHref0+_rwObsfuscatedHref1+_rwObsfuscatedHref2+_rwObsfuscatedHref3+_rwObsfuscatedHref4+_rwObsfuscatedHref5+_rwObsfuscatedHref6+_rwObsfuscatedHref7+_rwObsfuscatedHref8+_rwObsfuscatedHref9; document.getElementById('rw_email_contact').href = _rwObsfuscatedHref;</script></p>\n",
              "</div><!-- End Footer -->\n",
              "</div><!-- End container -->\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgQUIHlOV4qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}